import os
import shutil
import csv
import json
import re

# ================= é…ç½®åŒºåŸŸ (ä¿æŒä½ ä¹‹å‰çš„è·¯å¾„) =================
CSV_FILE = r"C:\GSD\SCI\Final\web_database.csv"

# è¯·ç¡®è®¤è¿™äº›è·¯å¾„æ˜¯å¯¹çš„
SRC_NOBG_ROOT = r"C:\GSD\SCI\Final\Final_Data_Packages\Photos_NoBG" 
SRC_DEPTH_ROOT = r"C:\GSD\SCI\Final\Final_Data_Packages\Depth"
SRC_PARTS_ROOT = r"C:\GSD\SCI\Final\Vase_Parts_Library_Drag" 

# è¾“å‡ºä½ç½®
DEST_ROOT = r"public/assets/images"
OUTPUT_JSON = "frontend_master_db.json"
# ===========================================

# æ¸…ç†ç›®æ ‡ç›®å½•
if os.path.exists(DEST_ROOT):
    try:
        shutil.rmtree(DEST_ROOT)
        print(f"ğŸ§¹ å·²æ¸…ç©ºæ—§æ–‡ä»¶å¤¹: {DEST_ROOT}")
    except:
        print("âš ï¸ æ— æ³•è‡ªåŠ¨æ¸…ç©ºæ–‡ä»¶å¤¹ï¼Œè¯·æ‰‹åŠ¨åˆ é™¤ public/assets/images åå†è¿è¡Œï¼")

for sub in ["original", "depth", "parts/neck", "parts/body", "parts/base"]:
    os.makedirs(os.path.join(DEST_ROOT, sub), exist_ok=True)

print("ğŸš€ å¼€å§‹å¸¦ã€é»‘åå•è¿‡æ»¤ã€‘çš„æ¬è¿...")

# è¯»å– CSV
if not os.path.exists(CSV_FILE):
    print("âŒ æ‰¾ä¸åˆ° CSV")
    exit()

with open(CSV_FILE, 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    csv_data = list(reader)

master_db = []
processed_count = 0

def sanitize_name(name):
    name = os.path.splitext(name)[0]
    name = re.sub(r'[()\s]+', '_', name)
    return name.strip('_')

# === å…³é”®ä¿®æ”¹ï¼šå¢åŠ  ignore_keywords å‚æ•° ===
def find_source_file(root_folder, filename_stem, ignore_keywords=None):
    if ignore_keywords is None:
        ignore_keywords = []
        
    for root, dirs, files in os.walk(root_folder):
        # 1. è¿‡æ»¤æ‰ä¸æƒ³è¿›å…¥çš„æ–‡ä»¶å¤¹ (æ¯”å¦‚ parts, neck, body)
        # è¿™ä¸€æ­¥èƒ½é˜²æ­¢è„šæœ¬è¯¯å…¥æ­§é€”
        dirs[:] = [d for d in dirs if not any(bad in d.lower() for bad in ignore_keywords)]
        
        # 2. æ£€æŸ¥å½“å‰è·¯å¾„æ˜¯å¦åŒ…å«å…³é”®è¯ (åŒé‡ä¿é™©)
        if any(bad in root.lower() for bad in ignore_keywords):
            continue

        for f in files:
            if os.path.splitext(f)[0] == filename_stem and f.lower().endswith(('.png', '.jpg', '.jpeg')):
                return os.path.join(root, f)
    return None

for row in csv_data:
    original_filename = row['filename']
    region = row['region']
    name_stem = os.path.splitext(original_filename)[0]
    
    clean_stem = sanitize_name(name_stem)
    unique_name = f"{region}_{clean_stem}.png"
    unique_id = f"{region}_{clean_stem}" # ID ä¹Ÿè¦åŠ ä¸Šåœ°åŒº
    
    # 1. æ¬è¿ Original (å…³é”®ï¼šå¿½ç•¥ parts æ–‡ä»¶å¤¹)
    # æˆ‘ä»¬å‘Šè¯‰è„šæœ¬ï¼šæ‰¾åŸå›¾æ—¶ï¼Œåƒä¸‡åˆ«å» neck, body, base é‡Œé¢æ‰¾ï¼
    src_img = find_source_file(SRC_NOBG_ROOT, name_stem, ignore_keywords=["part", "neck", "body", "base", "mask", "edge"])
    
    if src_img:
        shutil.copy(src_img, os.path.join(DEST_ROOT, "original", unique_name))
    else:
        continue # æ²¡åŸå›¾å°±è·³è¿‡

    # 2. æ¬è¿ Depth
    src_depth = find_source_file(SRC_DEPTH_ROOT, name_stem)
    if src_depth:
        shutil.copy(src_depth, os.path.join(DEST_ROOT, "depth", unique_name))
    
    # 3. æ¬è¿ Parts (è¿™é‡Œä¸éœ€è¦è¿‡æ»¤ï¼Œå› ä¸ºæˆ‘ä»¬æŒ‡å®šè¦å» parts æ–‡ä»¶å¤¹æ‰¾)
    parts_paths = {"neck": "", "body": "", "base": ""}
    for part_name in ["neck", "body", "base"]:
        part_src_root = os.path.join(SRC_PARTS_ROOT, part_name)
        if os.path.exists(part_src_root):
            src_part = find_source_file(part_src_root, name_stem)
            if src_part:
                shutil.copy(src_part, os.path.join(DEST_ROOT, f"parts/{part_name}", unique_name))
                parts_paths[part_name] = f"/assets/images/parts/{part_name}/{unique_name}"

    # 4. å†™å…¥ JSON
    entry = {
        "id": unique_id,
        "region": region,
        "globe_coordinates": { "x": float(row['x']), "y": float(row['y']) },
        "assets": {
            "image_url": f"/assets/images/original/{unique_name}",
            "depth_url": f"/assets/images/depth/{unique_name}",
            "parts": parts_paths
        }
    }
    master_db.append(entry)
    processed_count += 1
    
    if processed_count % 50 == 0:
        print(f"å·²å¤„ç† {processed_count} å¼ ...")

with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
    json.dump(master_db, f, indent=4)

print("-" * 30)
print(f"âœ… ä¿®å¤å®Œæˆï¼å…±å¤„ç†: {processed_count} å¼ ")
print("åˆ‡ç‰‡å¹²æ‰°å·²æ’é™¤ï¼Œç°åœ¨ Original æ–‡ä»¶å¤¹é‡Œåº”è¯¥å…¨æ˜¯å®Œæ•´çš„èŠ±ç“¶äº†ã€‚")