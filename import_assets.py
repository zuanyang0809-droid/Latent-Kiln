import os
import shutil
import json
import csv
import re
from pathlib import Path

# ================= é…ç½®åŒºåŸŸ =================

SOURCE_ROOT = r"C:\GSD\SCI\Final\Final_Data_Packages"
CSV_PATH = Path(r"C:\GSD\SCI\Final\web_database.csv")

DIR_NAME_ORIGINAL = "Photos_NoBG"
DIR_NAME_DEPTH = "Depth"
DIR_NAME_PARTS = "Parts"

PROJECT_ROOT = Path(__file__).resolve().parent

TARGET_ROOT = PROJECT_ROOT / "public" / "assets" / "images"
TARGET_ORIGINAL = TARGET_ROOT / "original"
TARGET_DEPTH = TARGET_ROOT / "depth"
TARGET_PARTS_NECK = TARGET_ROOT / "parts" / "neck"
TARGET_PARTS_BODY = TARGET_ROOT / "parts" / "body"
TARGET_PARTS_BASE = TARGET_ROOT / "parts" / "base"

DB_OUTPUT_PATH = PROJECT_ROOT / "frontend_master_db.json"

# ==========================================

def clean_string(name):
    clean = re.sub(r'[^\w]', '_', name).strip('_')
    clean = re.sub(r'_+', '_', clean)
    return clean

def load_csv_data(csv_path):
    data_map = {}
    if not csv_path.exists():
        return data_map
    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                filename = row.get('filename', '')
                if not filename: continue
                key = Path(filename).stem.lower().strip()
                try:
                    tx = float(row.get('x', 0))
                    ty = float(row.get('y', 0))
                except ValueError:
                    tx, ty = 0, 0
                data_map[key] = {
                    "region": row.get('region', 'Unknown'),
                    "x": tx,
                    "y": ty
                }
    except Exception:
        pass
    return data_map

def ensure_clean_dir(directory):
    if directory.exists():
        shutil.rmtree(directory)
    directory.mkdir(parents=True, exist_ok=True)

# === æ–°å¢ï¼šå»ºç«‹æ–‡ä»¶ç´¢å¼• ===
def build_file_index(directory):
    """
    éå†æŒ‡å®šç›®å½•ï¼Œå»ºç«‹ä¸€ä¸ªå­—å…¸ï¼š
    Key: æ–‡ä»¶å(ä¸å¸¦åç¼€ï¼Œå°å†™)
    Value: æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    """
    index = {}
    print(f"ğŸ” æ­£åœ¨ç´¢å¼•æ–‡ä»¶å¤¹: {directory.name} ...")
    if not directory.exists():
        print(f"âš ï¸ è­¦å‘Š: æ–‡ä»¶å¤¹ä¸å­˜åœ¨ {directory}")
        return index

    count = 0
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                # è·å–æ–‡ä»¶åä½œä¸º Key (æ¯”å¦‚ 'main-image_35')
                stem = Path(file).stem.lower().strip()
                # è®°å½•å®Œæ•´è·¯å¾„
                index[stem] = Path(root) / file
                count += 1
    print(f"   - ç´¢å¼•äº† {count} ä¸ªæ–‡ä»¶")
    return index

def main():
    print("ğŸš€ å¼€å§‹å…¨é‡åŒæ­¥ (å¿½ç•¥æ–‡ä»¶å¤¹ç»“æ„å·®å¼‚)...")

    csv_data = load_csv_data(CSV_PATH)
    path_root = Path(SOURCE_ROOT)
    
    # 1. å»ºç«‹ Depth å’Œ Parts çš„ç´¢å¼• (è¿™å°±æ˜¯é­”æ³•æ‰€åœ¨ï¼)
    # ä¸ç®¡æ·±åº¦å›¾åœ¨ Depth/test è¿˜æ˜¯ Depth/trainï¼Œåªè¦åå­—å¯¹ï¼Œå°±èƒ½æ‰¾åˆ°
    depth_index = build_file_index(path_root / DIR_NAME_DEPTH)
    
    parts_neck_index = build_file_index(path_root / DIR_NAME_PARTS / "neck")
    parts_body_index = build_file_index(path_root / DIR_NAME_PARTS / "body")
    parts_base_index = build_file_index(path_root / DIR_NAME_PARTS / "base")

    # 2. å‡†å¤‡ç›®å½•
    path_original = path_root / DIR_NAME_ORIGINAL
    if not path_original.exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºå›¾ç‰‡æ–‡ä»¶å¤¹: {path_original}")
        return

    print("ğŸ§¹ æ¸…ç† public æ–‡ä»¶å¤¹...")
    ensure_clean_dir(TARGET_ORIGINAL)
    ensure_clean_dir(TARGET_DEPTH)
    ensure_clean_dir(TARGET_PARTS_NECK)
    ensure_clean_dir(TARGET_PARTS_BODY)
    ensure_clean_dir(TARGET_PARTS_BASE)

    db_entries = []
    seen_ids = set()
    stats = {"processed": 0, "kept": 0, "dropped_no_depth": 0}

    # 3. éå†åŸå›¾
    for root, dirs, files in os.walk(path_original):
        # ä»ç„¶è®¡ç®—è·¯å¾„å‰ç¼€ï¼Œä¸ºäº†ç”Ÿæˆ ID
        rel_path_obj = Path(root).relative_to(path_original)
        rel_path_str = str(rel_path_obj)
        if rel_path_str == ".": path_prefix = ""
        else: path_prefix = clean_string(rel_path_str)

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                stats["processed"] += 1
                
                # è·å–æ ¸å¿ƒæ–‡ä»¶å (Key)
                file_stem = Path(file).stem.lower().strip()

                # === æ ¸å¿ƒä¿®æ”¹ï¼šé€šè¿‡ç´¢å¼•æŸ¥æ‰¾ Depth ===
                found_depth_src = depth_index.get(file_stem)
                
                if not found_depth_src:
                    # å¦‚æœåœ¨ Depth æ–‡ä»¶å¤¹çš„ä»»ä½•è§’è½éƒ½æ‰¾ä¸åˆ°åŒåæ–‡ä»¶ -> ä¸¢å¼ƒ
                    # print(f"ä¸¢å¼ƒ: {file} (æœªæ‰¾åˆ°å¯¹åº” Depth)") # è°ƒè¯•ç”¨
                    stats["dropped_no_depth"] += 1
                    continue
                # ===================================

                # A. CSV åŒ¹é…
                meta = csv_data.get(file_stem)
                if meta:
                    region = meta['region']
                    coord = {"x": meta['x'], "y": meta['y']}
                else:
                    folder_lower = os.path.basename(root).lower()
                    if 'africa' in folder_lower: region = 'Africa'
                    elif 'asia' in folder_lower: region = 'East Asia'
                    elif 'europe' in folder_lower: region = 'Europe'
                    elif 'americas' in folder_lower: region = 'Americas'
                    elif 'middle' in folder_lower: region = 'Middle East'
                    else: region = 'Unknown'
                    coord = {"x": 0, "y": 0}

                # B. ç”Ÿæˆ ID
                clean_name = clean_string(Path(file).stem)
                if path_prefix: new_id = f"{region}_{path_prefix}_{clean_name}"
                else: new_id = f"{region}_{clean_name}"

                unique_id = new_id
                counter = 1
                while unique_id in seen_ids:
                    unique_id = f"{new_id}_{counter}"
                    counter += 1
                seen_ids.add(unique_id)

                extension = Path(file).suffix
                new_filename = f"{unique_id}{extension}"

                # C. å¤åˆ¶ Original
                src_orig = Path(root) / file
                dst_orig = TARGET_ORIGINAL / new_filename
                shutil.copy2(src_orig, dst_orig)

                # D. å¤åˆ¶ Depth (ä½¿ç”¨ä»ç´¢å¼•é‡Œæ‰¾åˆ°çš„è·¯å¾„)
                dst_depth = TARGET_DEPTH / new_filename
                shutil.copy2(found_depth_src, dst_depth)
                final_depth_url = f"/assets/images/depth/{new_filename}"

                # E. æŸ¥æ‰¾ Parts (ä½¿ç”¨ç´¢å¼•æŸ¥æ‰¾)
                parts_urls = {"neck": "", "body": "", "base": ""}
                
                # Neck
                if file_stem in parts_neck_index:
                    shutil.copy2(parts_neck_index[file_stem], TARGET_PARTS_NECK / new_filename)
                    parts_urls["neck"] = f"/assets/images/parts/neck/{new_filename}"
                
                # Body
                if file_stem in parts_body_index:
                    shutil.copy2(parts_body_index[file_stem], TARGET_PARTS_BODY / new_filename)
                    parts_urls["body"] = f"/assets/images/parts/body/{new_filename}"
                
                # Base
                if file_stem in parts_base_index:
                    shutil.copy2(parts_base_index[file_stem], TARGET_PARTS_BASE / new_filename)
                    parts_urls["base"] = f"/assets/images/parts/base/{new_filename}"

                # F. å†™å…¥
                entry = {
                    "id": unique_id,
                    "region": region,
                    "period": "Unknown",
                    "globe_coordinates": coord,
                    "assets": {
                        "image_url": f"/assets/images/original/{new_filename}",
                        "depth_url": final_depth_url,
                        "parts": parts_urls
                    }
                }
                db_entries.append(entry)
                stats["kept"] += 1

    print("-" * 30)
    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   - æ‰«æåŸå›¾: {stats['processed']}")
    print(f"   - âŒ æ— Depthä¸¢å¼ƒ: {stats['dropped_no_depth']}")
    print(f"   - âœ… æœ€ç»ˆä¿ç•™: {stats['kept']}")
    
    with open(DB_OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(db_entries, f, ensure_ascii=False, indent=2)
        
    print(f"ğŸ“ æ•°æ®åº“å·²ç”Ÿæˆ: {DB_OUTPUT_PATH}")

if __name__ == "__main__":
    main()