import os
import shutil
import json
import csv
import re
import hashlib  # <--- æ–°å¢ï¼šç”¨äºè®¡ç®—æ–‡ä»¶æŒ‡çº¹
from pathlib import Path

# ================= é…ç½®åŒºåŸŸ =================

SOURCE_ROOT = r"C:\GSD\SCI\Final\Final_Data_Packages"
CSV_PATH = Path(r"C:\GSD\SCI\Final\web_database.csv")

DIR_NAME_ORIGINAL = "Photos_NoBG"
DIR_NAME_DEPTH = "Depth"
DIR_NAME_PARTS = "Parts"

PROJECT_ROOT = Path(__file__).resolve().parent

TARGET_ROOT = PROJECT_ROOT / "public" / "assets" / "images"
TARGET_ORIGINAL = TARGET_ROOT / "original"
TARGET_DEPTH = TARGET_ROOT / "depth"
TARGET_PARTS_NECK = TARGET_ROOT / "parts" / "neck"
TARGET_PARTS_BODY = TARGET_ROOT / "parts" / "body"
TARGET_PARTS_BASE = TARGET_ROOT / "parts" / "base"

DB_OUTPUT_PATH = PROJECT_ROOT / "frontend_master_db.json"

# ==========================================

def calculate_file_hash(filepath):
    """
    è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼ï¼ˆæ•°å­—æŒ‡çº¹ï¼‰ã€‚
    å¦‚æœä¸¤å¼ å›¾å†…å®¹å®Œå…¨ä¸€æ ·ï¼Œå“ˆå¸Œå€¼å°±ä¸€æ ·ã€‚
    """
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        # åˆ†å—è¯»å–ï¼Œé˜²æ­¢å¤§æ–‡ä»¶æ’‘çˆ†å†…å­˜
        buf = f.read(65536)
        while len(buf) > 0:
            hasher.update(buf)
            buf = f.read(65536)
    return hasher.hexdigest()

def clean_string(name):
    clean = re.sub(r'[^\w]', '_', name).strip('_')
    clean = re.sub(r'_+', '_', clean)
    return clean

def load_csv_data(csv_path):
    data_map = {}
    if not csv_path.exists():
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ‰¾ä¸åˆ° CSV æ–‡ä»¶: {csv_path}")
        return data_map

    print(f"ğŸ“– æ­£åœ¨è¯»å– CSV: {csv_path}")
    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                filename = row.get('filename', '')
                if not filename: continue
                key = Path(filename).stem.lower().strip()
                try:
                    tx = float(row.get('x', 0))
                    ty = float(row.get('y', 0))
                except ValueError:
                    tx, ty = 0, 0
                
                entry = {
                    "region": row.get('region', 'Unknown'),
                    "x": tx,
                    "y": ty
                }
                
                if key not in data_map:
                    data_map[key] = []
                data_map[key].append(entry)
                
    except Exception as e:
        print(f"âŒ è¯»å– CSV å¤±è´¥: {e}")
    
    print(f"âœ… CSV è¯»å–å®Œæ¯•ï¼ŒåŒ…å« {len(data_map)} ä¸ªå”¯ä¸€æ–‡ä»¶åç´¢å¼•ã€‚")
    return data_map

def ensure_clean_dir(directory):
    if directory.exists():
        shutil.rmtree(directory)
    directory.mkdir(parents=True, exist_ok=True)

def build_file_index(directory):
    index = {}
    if not directory.exists():
        return index
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                stem = Path(file).stem.lower().strip()
                index[stem] = Path(root) / file
    return index

def determine_region_from_folder(folder_name):
    lower = folder_name.lower()
    if 'africa' in lower: return 'Africa'
    if 'east asia' in lower or 'east_asia' in lower: return 'East Asia'
    if 'asia' in lower: return 'East Asia'
    if 'europe' in lower: return 'Europe'
    if 'americas' in lower or 'america' in lower: return 'Americas'
    if 'middle' in lower: return 'Middle East'
    return 'Unknown'

def main():
    print("ğŸš€ å¼€å§‹å…¨é‡åŒæ­¥ (åŸºäºå†…å®¹å»é‡ + CSV åŒ¹é…)...")

    # 1. è¯»å– CSV
    csv_data_map = load_csv_data(CSV_PATH)

    # 2. å»ºç«‹è¾…åŠ©ç´¢å¼•
    path_root = Path(SOURCE_ROOT)
    depth_index = build_file_index(path_root / DIR_NAME_DEPTH)
    parts_neck_index = build_file_index(path_root / DIR_NAME_PARTS / "neck")
    parts_body_index = build_file_index(path_root / DIR_NAME_PARTS / "body")
    parts_base_index = build_file_index(path_root / DIR_NAME_PARTS / "base")

    # 3. å‡†å¤‡ç›®å½•
    path_original = path_root / DIR_NAME_ORIGINAL
    if not path_original.exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºå›¾ç‰‡æ–‡ä»¶å¤¹: {path_original}")
        return

    print("ğŸ§¹ æ¸…ç† public æ–‡ä»¶å¤¹...")
    ensure_clean_dir(TARGET_ORIGINAL)
    ensure_clean_dir(TARGET_DEPTH)
    ensure_clean_dir(TARGET_PARTS_NECK)
    ensure_clean_dir(TARGET_PARTS_BODY)
    ensure_clean_dir(TARGET_PARTS_BASE)

    db_entries = []
    seen_ids = set()
    
    # === æ–°å¢ï¼šç”¨äºè®°å½•å·²å¤„ç†å›¾ç‰‡çš„å“ˆå¸Œå€¼ ===
    seen_image_hashes = set()
    
    stats = {
        "processed": 0, 
        "kept": 0, 
        "dropped_no_depth": 0, 
        "dropped_content_duplicate": 0, # æ–°å¢ç»Ÿè®¡
        "exact_match": 0, 
        "fallback_coord": 0
    }

    # 4. éå†åŸå›¾
    for root, dirs, files in os.walk(path_original):
        rel_path_obj = Path(root).relative_to(path_original)
        rel_path_str = str(rel_path_obj)
        if rel_path_str == ".": path_prefix = ""
        else: path_prefix = clean_string(rel_path_str)

        current_folder_region = determine_region_from_folder(os.path.basename(root))

        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                stats["processed"] += 1
                
                src_orig = Path(root) / file
                file_stem = Path(file).stem.lower().strip()

                # --- 0. æ ¸å¿ƒæ–°å¢ï¼šåŸºäºå›¾ç‰‡å†…å®¹å»é‡ ---
                # è®¡ç®—å½“å‰å›¾ç‰‡çš„æŒ‡çº¹
                file_hash = calculate_file_hash(src_orig)
                
                if file_hash in seen_image_hashes:
                    # æŒ‡çº¹å·²å­˜åœ¨ï¼Œè¯´æ˜æ˜¯å®Œå…¨ä¸€æ ·çš„å›¾ç‰‡
                    stats["dropped_content_duplicate"] += 1
                    # print(f"âš ï¸ å‘ç°å†…å®¹é‡å¤å›¾ç‰‡ï¼Œè·³è¿‡: {file}") # è°ƒè¯•ç”¨
                    continue
                
                # è®°å½•æ–°æŒ‡çº¹
                seen_image_hashes.add(file_hash)
                # ----------------------------------

                # --- 1. å¿…é¡»æœ‰ Depth ---
                found_depth_src = depth_index.get(file_stem)
                if not found_depth_src:
                    stats["dropped_no_depth"] += 1
                    continue

                # --- 2. åŒ¹é… CSV ---
                matched_csv_entry = None
                potential_entries = csv_data_map.get(file_stem, [])

                if len(potential_entries) == 1:
                    matched_csv_entry = potential_entries[0]
                elif len(potential_entries) > 1:
                    for entry in potential_entries:
                        if entry['region'] == current_folder_region:
                            matched_csv_entry = entry
                            break
                    if not matched_csv_entry:
                        matched_csv_entry = potential_entries[0]
                
                if matched_csv_entry:
                    region = matched_csv_entry['region']
                    coord = {"x": matched_csv_entry['x'], "y": matched_csv_entry['y']}
                    stats["exact_match"] += 1
                else:
                    region = current_folder_region
                    coord = {"x": 0, "y": 0}
                    stats["fallback_coord"] += 1

                # --- 3. ç”Ÿæˆ ID ---
                clean_name = clean_string(Path(file).stem)
                if path_prefix: new_id = f"{region}_{path_prefix}_{clean_name}"
                else: new_id = f"{region}_{clean_name}"

                unique_id = new_id
                counter = 1
                while unique_id in seen_ids:
                    unique_id = f"{new_id}_{counter}"
                    counter += 1
                seen_ids.add(unique_id)

                extension = Path(file).suffix
                new_filename = f"{unique_id}{extension}"

                # --- 4. å¤åˆ¶æ–‡ä»¶ ---
                dst_orig = TARGET_ORIGINAL / new_filename
                shutil.copy2(src_orig, dst_orig)

                dst_depth = TARGET_DEPTH / new_filename
                shutil.copy2(found_depth_src, dst_depth)
                final_depth_url = f"/assets/images/depth/{new_filename}"

                # --- 5. æŸ¥æ‰¾ Parts ---
                parts_urls = {"neck": "", "body": "", "base": ""}
                
                if file_stem in parts_neck_index:
                    shutil.copy2(parts_neck_index[file_stem], TARGET_PARTS_NECK / new_filename)
                    parts_urls["neck"] = f"/assets/images/parts/neck/{new_filename}"
                
                if file_stem in parts_body_index:
                    shutil.copy2(parts_body_index[file_stem], TARGET_PARTS_BODY / new_filename)
                    parts_urls["body"] = f"/assets/images/parts/body/{new_filename}"
                
                if file_stem in parts_base_index:
                    shutil.copy2(parts_base_index[file_stem], TARGET_PARTS_BASE / new_filename)
                    parts_urls["base"] = f"/assets/images/parts/base/{new_filename}"

                # --- 6. å†™å…¥ ---
                entry = {
                    "id": unique_id,
                    "region": region,
                    "period": "Unknown",
                    "globe_coordinates": coord,
                    "assets": {
                        "image_url": f"/assets/images/original/{new_filename}",
                        "depth_url": final_depth_url,
                        "parts": parts_urls
                    }
                }
                db_entries.append(entry)
                stats["kept"] += 1

    print("-" * 30)
    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   - æ‰«æåŸå›¾: {stats['processed']}")
    print(f"   - ğŸš« æ— Depthä¸¢å¼ƒ: {stats['dropped_no_depth']}")
    print(f"   - âœ‚ï¸  å†…å®¹é‡å¤ä¸¢å¼ƒ: {stats['dropped_content_duplicate']} (MD5 å»é‡)")
    print(f"   - âœ… æœ€ç»ˆä¿ç•™: {stats['kept']}")
    print(f"     ----------------")
    print(f"     - ç²¾å‡†åŒ¹é… CSV: {stats['exact_match']}")
    print(f"     - æœªåŒ¹é… CSV (0,0): {stats['fallback_coord']}")
    
    with open(DB_OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(db_entries, f, ensure_ascii=False, indent=2)
        
    print(f"ğŸ“ æ•°æ®åº“å·²ç”Ÿæˆ: {DB_OUTPUT_PATH}")

if __name__ == "__main__":
    main()